{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code extracts the names of malls in Singapore from two sources:\n",
    "1. Wikipedia Web Scraping – Mall names are extracted by scraping Wikipedia.\n",
    "2. Google Places Text Search – Mall names are retrieved using the Google Places API.\n",
    "\n",
    "Once the mall names are collected from both sources, the code queries the OneMap API to obtain additional details such as address, longitude, and latitude for each mall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Google Places API Set-Up\n",
    "GOOGLE_API_KEY = \"AIzaSyDpu7X3vaLLr2GhCX6BcNWhfUtcJwU8F-A\"\n",
    "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "GEOCODE_URL = \"https://maps.googleapis.com/maps/api/geocode/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- OneMap API Set-Up\n",
    "ONEMAP_BASE_URL = \"https://www.onemap.gov.sg/api/common/elastic/search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrape from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wiki details set-up\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore\"\n",
    "wiki_response = requests.get(wiki_url, timeout = 5)\n",
    "content = BeautifulSoup(wiki_response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- 1. Extract mall names from main section\n",
    "mall_names_main_section = []\n",
    "\n",
    "# Mall names can be found under each region header\n",
    "regions = content.find_all(\"h2\")\n",
    "\n",
    "for region in regions:\n",
    "    region_text = region.get_text(strip=True)\n",
    "\n",
    "    # Skip 'Contents' and 'References' Section\n",
    "    if region_text in [\"Contents\", \"References\"]:\n",
    "        continue\n",
    "\n",
    "    div_col = region.find_next(\"div\", class_=\"div-col\")\n",
    "    \n",
    "    if div_col:\n",
    "        malls = div_col.find_all(\"li\")  # Extract malls from <li> tags\n",
    "\n",
    "        for mall in malls:\n",
    "            # Some malls are in <a> tags, some are plain text\n",
    "            mall_name = mall.get_text(strip=True)\n",
    "            if mall_name:\n",
    "                mall_names_main_section.append(mall_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- 2. Extract mall names from references section (Current malls)\n",
    "\n",
    "# Find the box containing current mall names\n",
    "mall_names_ref_section = []\n",
    "wiki_ref_mall_section = content.find(\"td\", class_=\"navbox-list-with-group navbox-list navbox-odd hlist\")\n",
    "\n",
    "if wiki_ref_mall_section:\n",
    "    # Locate only the <ul> lists inside the section to avoid the headers\n",
    "    for ul in wiki_ref_mall_section.find_all(\"ul\"):\n",
    "        # Extract mall names from <a> tags\n",
    "        for a in ul.find_all(\"a\"):\n",
    "            mall_name = a.text.strip()\n",
    "            mall_names_ref_section.append(mall_name)\n",
    "else: \n",
    "    print(\"Current malls section under references not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- 3. Combine mall names obtained from both sections to get a complete list\n",
    "\n",
    "combined_mall_names = mall_names_main_section.copy()\n",
    "combined_mall_names.extend(mall_names_ref_section)\n",
    "combined_mall_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- 4. Cleaning of web-scrapped data\n",
    "cleansed_wiki_mall_names = combined_mall_names.copy()\n",
    "\n",
    "#--  Standardise all to be uppercase\n",
    "cleansed_wiki_mall_names = [mall.upper() for mall in cleansed_wiki_mall_names]\n",
    "\n",
    "#-- Remove whitespace\n",
    "cleansed_wiki_mall_names = [mall.strip() for mall in cleansed_wiki_mall_names]\n",
    "\n",
    "#-- Clean mall names \n",
    "# e.g. GRID (FORMERLY POMO)[1], KINEX (FORMERLY ONEKM)\n",
    "def clean_mall_names(dataset):\n",
    "    result = []\n",
    "    for i in dataset:\n",
    "        # Remove content inside square brackets [ ] including the brackets\n",
    "        i = re.sub(r\"\\[.*?\\]\", \"\", i)\n",
    "        # Remove the bracketed part for 'formerly' cases\n",
    "        i = re.sub(r\"\\s*\\(FORMERLY .*?\\)\", \"\", i, flags=re.IGNORECASE)\n",
    "        result.append(i.strip())\n",
    "    return result\n",
    "\n",
    "cleansed_wiki_mall_names = clean_mall_names(cleansed_wiki_mall_names)\n",
    "\n",
    "#-- Remove duplicates\n",
    "def remove_duplicates(mall_list):\n",
    "    result = []\n",
    "    for i in mall_list:\n",
    "        if i not in result:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "cleansed_wiki_mall_names = remove_duplicates(cleansed_wiki_mall_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- 5. Manual Cleaning of web-scrapped data\n",
    "wiki_replacement_dict = {\n",
    "    'PAYA LEBAR QUARTER (PLQ)': 'PLQ MALL',\n",
    "    'THE PARAGON': 'PARAGON SHOPPING CENTRE',\n",
    "    'DJITSUN MALL BEDOK': 'DJITSUN MALL'\n",
    "}\n",
    "cleansed_wiki_mall_names = [wiki_replacement_dict.get(mall, mall) for mall in cleansed_wiki_mall_names]\n",
    "\n",
    "wiki_malls_to_remove = {'TENGAH MALL (2027)', 'FAIRPRICE HUB', 'MARINA BAY SANDS', 'HOLLAND VILLAGE SHOPPING MALL', 'OD MALL', 'HOUGANG GREEN SHOPPING MALL'}\n",
    "cleansed_wiki_mall_names = [mall for mall in cleansed_wiki_mall_names if mall not in wiki_malls_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mall_details_onemap(mall_name):\n",
    "    params = {\n",
    "        'searchVal': mall_name,\n",
    "        'returnGeom': 'Y',\n",
    "        'getAddrDetails': 'Y'\n",
    "    }\n",
    "\n",
    "    response = requests.get(ONEMAP_BASE_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200: \n",
    "        data = response.json()\n",
    "        if data['found'] > 0:\n",
    "            result = data['results'][0]\n",
    "            return {\n",
    "                'mall_name': mall_name,\n",
    "                # 'onemap_mall_name': result['BUILDING'],\n",
    "                'address': result['ADDRESS'],\n",
    "                'latitude': result['LATITUDE'],\n",
    "                'longitude': result['LONGITUDE']\n",
    "            }\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_mall_details = []\n",
    "for mall in cleansed_wiki_mall_names:\n",
    "    mall_info = get_mall_details_onemap(mall)\n",
    "    if mall_info:\n",
    "        wiki_mall_details.append(mall_info)\n",
    "df_malls_wiki = pd.DataFrame(wiki_mall_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Places - Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mall_names_text_search():\n",
    "    malls = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"query\": \"shopping malls in Singapore\",\n",
    "            \"region\": \"sg\",\n",
    "            \"key\": GOOGLE_API_KEY\n",
    "        }\n",
    "        if next_page_token:\n",
    "            params[\"pagetoken\"] = next_page_token\n",
    "\n",
    "        response = requests.get(TEXT_SEARCH_URL, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"results\" in data:\n",
    "            for result in data[\"results\"]:\n",
    "                malls.append(result.get(\"name\", \"\"))  # Only store the name\n",
    "\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        time.sleep(3)  # To handle rate limits\n",
    "\n",
    "    return malls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_mall_names = get_mall_names_text_search()\n",
    "\n",
    "# Manual cleaning\n",
    "text_search_malls_to_remove = ['CLARKE QUAY', 'UNITED SQUARE SHOPPING MALL', 'GREAT WORLD CITY', 'IMM BUILDING', 'TAKASHIMAYA SHOPPING CENTRE']\n",
    "text_search_mall_names = [mall.upper() for mall in text_search_mall_names]\n",
    "text_search_mall_names = [mall for mall in text_search_mall_names if mall not in text_search_malls_to_remove]\n",
    "\n",
    "text_search_mall_details = [get_mall_details_onemap(mall) for mall in text_search_mall_names]\n",
    "df_malls_text_search = pd.DataFrame(text_search_mall_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_malls_text_search, df_malls_wiki], ignore_index=True)\n",
    "df_combined = df_combined.sort_values(by='mall_name')\n",
    "df_combined.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'the', 'trailing spaces', 'make mall names uppercase'\n",
    "df_combined['std_mall_name'] = df_combined['mall_name'].str.replace(r'\\bthe\\b', '', case=False, regex=True).str.strip().str.upper()\n",
    "\n",
    "# Remove duplicates based on 'std_mall_name' column\n",
    "df_combined.drop_duplicates(subset='std_mall_name', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 4\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing address information that cannot be found on OneMap API\n",
    "def has_valid_postal_code(address):\n",
    "    return bool(re.search(r'\\b\\d{6}\\b', str(address)))  # Ensure it's a 6-digit number\n",
    "\n",
    "# Filter rows based on postal code presence\n",
    "initial_rows = df_combined.shape[0]\n",
    "df_combined = df_combined[df_combined['address'].apply(has_valid_postal_code)]\n",
    "final_rows = df_combined.shape[0]\n",
    "\n",
    "print(f\"Number of rows dropped: {initial_rows - final_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.drop(columns={'std_mall_name'}, inplace=True)\n",
    "df_combined = df_combined.map(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "df_combined.sort_values('mall_name', inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
